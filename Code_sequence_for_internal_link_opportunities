{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anubrag/test/blob/main/Code_sequence_for_internal_link_opportunities\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_bbWvxYmWYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7b3b1876-99b8-4c15-d99e-2f41672f999a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scann\n",
            "  Using cached scann-1.3.3-cp310-cp310-manylinux_2_27_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: tensorflow~=2.17.0 in /usr/local/lib/python3.10/dist-packages (from scann) (2.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scann) (1.26.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.17.0->scann) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow~=2.17.0->scann) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow~=2.17.0->scann) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow~=2.17.0->scann) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.17.0->scann) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.17.0->scann) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.17.0->scann) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.17.0->scann) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->scann) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->scann) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->scann) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow~=2.17.0->scann) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow~=2.17.0->scann) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow~=2.17.0->scann) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow~=2.17.0->scann) (0.1.2)\n",
            "Using cached scann-1.3.3-cp310-cp310-manylinux_2_27_x86_64.whl (10.7 MB)\n",
            "Installing collected packages: scann\n",
            "Successfully installed scann-1.3.3\n",
            "Requirement already satisfied: pyvis in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1.4)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.2.2)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (71.0.4)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9.6->pyvis) (2.1.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.13)\n",
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.10/dist-packages (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install necessary libraries\n",
        "\n",
        "!pip install scann\n",
        "!pip install pyvis\n",
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import necessary libraries, modules or components\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scann\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from google.colab import files\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from pyvis.network import Network\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "y3yz8KF4dklV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Upload and load the Screaming Frog file\n",
        "\n",
        "uploaded = files.upload()\n",
        "file_name = 'Minuttia Content Inventory.xlsx'\n",
        "siteDf = pd.read_excel(file_name)\n",
        "\n",
        "print(\"Displaying the first 5 rows of the DataFrame:\")\n",
        "print(siteDf.head())\n",
        "\n",
        "print(\"Column names in the DataFrame:\")\n",
        "print(siteDf.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "OvH_Ueqmds9d",
        "outputId": "a6037fcc-f24a-4563-9ef4-599c33d946dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6c2ba011-37bd-4518-9be4-edcee5d7a19b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6c2ba011-37bd-4518-9be4-edcee5d7a19b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Minuttia Content Inventory.xlsx to Minuttia Content Inventory.xlsx\n",
            "Displaying the first 5 rows of the DataFrame:\n",
            "                                        Original Url  \\\n",
            "0              https://minuttia.com/post-sitemap.xml   \n",
            "1                         https://minuttia.com/blog/   \n",
            "2                    https://minuttia.com/new-brand/   \n",
            "3  https://minuttia.com/wp-content/uploads/2022/1...   \n",
            "4            https://minuttia.com/keyword-selection/   \n",
            "\n",
            "                                   Address              Content Type  \\\n",
            "0    https://minuttia.com/post-sitemap.xml   text/xml; charset=UTF-8   \n",
            "1                                      NaN                       NaN   \n",
            "2          https://minuttia.com/new-brand/  text/html; charset=UTF-8   \n",
            "3                                      NaN                       NaN   \n",
            "4  https://minuttia.com/keyword-selection/  text/html; charset=UTF-8   \n",
            "\n",
            "   Status Code Status   Indexability Indexability Status  \\\n",
            "0        200.0     OK  Non-Indexable             noindex   \n",
            "1          NaN    NaN            NaN                 NaN   \n",
            "2        200.0    NaN      Indexable                 NaN   \n",
            "3          NaN    NaN            NaN                 NaN   \n",
            "4        200.0    NaN      Indexable                 NaN   \n",
            "\n",
            "                                             Title 1  Title 1 Length  \\\n",
            "0                                                NaN             0.0   \n",
            "1                                                NaN             NaN   \n",
            "2  Why (& How) We Built Minuttia's New Brand - Mi...            52.0   \n",
            "3                                                NaN             NaN   \n",
            "4  Keyword Selection at Scale Using Weighted Vari...            51.0   \n",
            "\n",
            "   Title 1 Pixel Width  ... Last Modified  Redirect URL  Redirect Type  \\\n",
            "0                  0.0  ...           NaN           NaN            NaN   \n",
            "1                  NaN  ...           NaN           NaN            NaN   \n",
            "2                477.0  ...           NaN           NaN            NaN   \n",
            "3                  NaN  ...           NaN           NaN            NaN   \n",
            "4                476.0  ...           NaN           NaN            NaN   \n",
            "\n",
            "   Cookies  Language HTTP Version  \\\n",
            "0      NaN       NaN          NaN   \n",
            "1      NaN       NaN          NaN   \n",
            "2      NaN     en-US          2.0   \n",
            "3      NaN       NaN          NaN   \n",
            "4      NaN     en-US          2.0   \n",
            "\n",
            "    (ChatGPT) Extract embeddings from page content 1 Mobile Alternate Link  \\\n",
            "0                                                NaN                   NaN   \n",
            "1                                                NaN                   NaN   \n",
            "2  0.00140454,-0.0036019366,0.01184621,0.01736391...                   NaN   \n",
            "3                                                NaN                   NaN   \n",
            "4  0.009092556,0.010230032,0.025864882,0.00217533...                   NaN   \n",
            "\n",
            "                       URL Encoded Address      Crawl Timestamp  \n",
            "0    https://minuttia.com/post-sitemap.xml  2024-06-17 17:24:12  \n",
            "1                                      NaN                  NaN  \n",
            "2          https://minuttia.com/new-brand/  2024-06-17 17:24:33  \n",
            "3                                      NaN                  NaN  \n",
            "4  https://minuttia.com/keyword-selection/  2024-06-17 17:24:39  \n",
            "\n",
            "[5 rows x 72 columns]\n",
            "Column names in the DataFrame:\n",
            "Index(['Original Url', 'Address', 'Content Type', 'Status Code', 'Status',\n",
            "       'Indexability', 'Indexability Status', 'Title 1', 'Title 1 Length',\n",
            "       'Title 1 Pixel Width', 'Meta Description 1',\n",
            "       'Meta Description 1 Length', 'Meta Description 1 Pixel Width',\n",
            "       'Meta Keywords 1', 'Meta Keywords 1 Length', 'H1-1', 'H1-1 Length',\n",
            "       'H1-2', 'H1-2 Length', 'H2-1', 'H2-1 Length', 'H2-2', 'H2-2 Length',\n",
            "       'Meta Robots 1', 'X-Robots-Tag 1', 'Meta Refresh 1',\n",
            "       'Canonical Link Element 1', 'rel=\"next\" 1', 'rel=\"prev\" 1',\n",
            "       'HTTP rel=\"next\" 1', 'HTTP rel=\"prev\" 1', 'amphtml Link Element',\n",
            "       'Size (bytes)', 'Transferred (bytes)', 'Total Transferred (bytes)',\n",
            "       'CO2 (mg)', 'Carbon Rating', 'Word Count', 'Sentence Count',\n",
            "       'Average Words Per Sentence', 'Flesch Reading Ease Score',\n",
            "       'Readability', 'Text Ratio', 'Crawl Depth', 'Folder Depth',\n",
            "       'Link Score', 'Inlinks', 'Unique Inlinks', 'Unique JS Inlinks',\n",
            "       '% of Total', 'Outlinks', 'Unique Outlinks', 'Unique JS Outlinks',\n",
            "       'External Outlinks', 'Unique External Outlinks',\n",
            "       'Unique External JS Outlinks', 'Closest Similarity Match',\n",
            "       'No. Near Duplicates', 'Spelling Errors', 'Grammar Errors', 'Hash',\n",
            "       'Response Time', 'Last Modified', 'Redirect URL', 'Redirect Type',\n",
            "       'Cookies', 'Language', 'HTTP Version',\n",
            "       '(ChatGPT) Extract embeddings from page content 1',\n",
            "       'Mobile Alternate Link', 'URL Encoded Address', 'Crawl Timestamp'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Process embeddings in Screaming Frog file\n",
        "\n",
        "column_name = '(ChatGPT) Extract embeddings from page content 1'\n",
        "\n",
        "# Filter out rows with invalid embeddings\n",
        "valid_rows = siteDf[column_name].apply(lambda x: isinstance(x, str) and 'Error' not in x)\n",
        "siteDf = siteDf[valid_rows]\n",
        "\n",
        "# Split and convert embeddings\n",
        "siteDf[column_name + 'AsFloats'] = siteDf[column_name].str.split(',')\n",
        "siteDf[column_name + 'AsFloats'] = siteDf[column_name + 'AsFloats'].apply(lambda x: np.array(x, dtype=np.float64))\n",
        "siteDf['EmbeddingLength'] = siteDf[column_name + 'AsFloats'].apply(lambda x: x.size)\n",
        "\n",
        "# Check for consistent embedding dimensions\n",
        "if len(siteDf['EmbeddingLength'].unique()) == 1:\n",
        "    d = siteDf['EmbeddingLength'].unique()[0]\n",
        "    print(f\"All embeddings have the same dimension: {d}\")\n",
        "else:\n",
        "    raise ValueError(\"Dimensionality reduction required to make all arrays the same size.\")\n",
        "\n",
        "# Create dataset and queries\n",
        "dataset = np.vstack(siteDf[column_name + 'AsFloats'].values)\n",
        "queries = dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "expe2NVaeGan",
        "outputId": "f0e9ae28-43a2-4d99-c6a0-eb8cff50e7e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All embeddings have the same dimension: 1536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-7e1cf4338a81>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  siteDf[column_name + 'AsFloats'] = siteDf[column_name].str.split(',')\n",
            "<ipython-input-5-7e1cf4338a81>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  siteDf[column_name + 'AsFloats'] = siteDf[column_name + 'AsFloats'].apply(lambda x: np.array(x, dtype=np.float64))\n",
            "<ipython-input-5-7e1cf4338a81>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  siteDf['EmbeddingLength'] = siteDf[column_name + 'AsFloats'].apply(lambda x: x.size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define ScaNN functions\n",
        "\n",
        "def scann_search(dataset: np.ndarray, queries: np.ndarray, n_neighbors=10, distance_measure=\"dot_product\", num_leaves=10, num_leaves_to_search=5):\n",
        "    normalized_dataset = dataset / np.linalg.norm(dataset, axis=1)[:, np.newaxis]\n",
        "    searcher = scann.scann_ops_pybind.builder(normalized_dataset, n_neighbors, distance_measure).tree(\n",
        "        num_leaves=num_leaves, num_leaves_to_search=num_leaves_to_search, training_sample_size=250000).score_ah(\n",
        "        2, anisotropic_quantization_threshold=0.2).reorder(100).build()\n",
        "    return searcher\n",
        "\n",
        "def convert_scann_arrays_to_urls(arrays: np.array, df: pd.DataFrame, column):\n",
        "    results = []\n",
        "    for arr in arrays:\n",
        "        results.append(df.iloc[arr.flatten()][column].tolist())\n",
        "    return results"
      ],
      "metadata": {
        "id": "4WLM700SemG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Run ScaNN search and serialize the index\n",
        "\n",
        "output_dir = 'site_scann_index'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "siteSearcher = scann_search(dataset, queries)\n",
        "siteSearcher.serialize(output_dir)"
      ],
      "metadata": {
        "id": "sAOpOPjGesMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Compute cosine similarity\n",
        "\n",
        "document_embeddings = np.vstack(siteDf[column_name + 'AsFloats'].values)\n",
        "cosine_sim_matrix = cosine_similarity(document_embeddings)\n",
        "\n",
        "# Flatten the matrix and get pairs\n",
        "similarity_pairs = [(i, j, cosine_sim_matrix[i, j]) for i in range(len(cosine_sim_matrix)) for j in range(i+1, len(cosine_sim_matrix))]\n",
        "\n",
        "# Sort pairs by similarity score\n",
        "similarity_pairs = sorted(similarity_pairs, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "# Create a DataFrame to hold the results\n",
        "similarity_df = pd.DataFrame(similarity_pairs, columns=['Doc1', 'Doc2', 'CosineSimilarity'])\n",
        "\n",
        "# Add URL columns for document pairs\n",
        "similarity_df['URL1'] = similarity_df['Doc1'].apply(lambda x: siteDf.iloc[x]['Original Url'])\n",
        "similarity_df['URL2'] = similarity_df['Doc2'].apply(lambda x: siteDf.iloc[x]['Original Url'])\n",
        "\n",
        "# Display the top 10 most similar document pairs\n",
        "print(\"Top 10 most similar document pairs:\")\n",
        "print(similarity_df.head(10))"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPn3PlMnMtVp",
        "outputId": "9e01e780-bc8b-4e24-874e-f96af7c2eda6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most similar document pairs:\n",
            "   Doc1  Doc2  CosineSimilarity  \\\n",
            "0    42    52          0.965695   \n",
            "1    24    30          0.960598   \n",
            "2    14    38          0.958372   \n",
            "3    26    45          0.953977   \n",
            "4     4    18          0.947114   \n",
            "5    24    52          0.945056   \n",
            "6    23    51          0.943519   \n",
            "7    16    38          0.942557   \n",
            "8     1    21          0.940844   \n",
            "9    31    51          0.940457   \n",
            "\n",
            "                                                URL1  \\\n",
            "0    https://minuttia.com/rule-of-content-marketing/   \n",
            "1  https://minuttia.com/content-marketing-is-chan...   \n",
            "2      https://minuttia.com/topical-authority-ratio/   \n",
            "3               https://minuttia.com/b2b-newsletter/   \n",
            "4    https://minuttia.com/comparison-keywords-study/   \n",
            "5  https://minuttia.com/content-marketing-is-chan...   \n",
            "6       https://minuttia.com/ai-content-predictions/   \n",
            "7           https://minuttia.com/topics-vs-keywords/   \n",
            "8            https://minuttia.com/keyword-selection/   \n",
            "9   https://minuttia.com/state-of-content-marketing/   \n",
            "\n",
            "                                                URL2  \n",
            "0  https://minuttia.com/b2b-content-marketing-str...  \n",
            "1     https://minuttia.com/content-marketing-budget/  \n",
            "2            https://minuttia.com/topical-authority/  \n",
            "3              https://minuttia.com/growthwaves-pro/  \n",
            "4   https://minuttia.com/alternative-keywords-study/  \n",
            "5  https://minuttia.com/b2b-content-marketing-str...  \n",
            "6                    https://minuttia.com/ai-policy/  \n",
            "7            https://minuttia.com/topical-authority/  \n",
            "8              https://minuttia.com/keyword-mapping/  \n",
            "9                    https://minuttia.com/ai-policy/  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Prepare a table with internal linking opportunities, handling large datasets\n",
        "\n",
        "# Filter out rows with CosineSimilarity of 0.99 and remove duplicates\n",
        "similarity_table = similarity_df[['URL1', 'URL2', 'CosineSimilarity']].copy()\n",
        "similarity_table = similarity_table[similarity_table['CosineSimilarity'] < 0.99]\n",
        "similarity_table.drop_duplicates(inplace=True)\n",
        "\n",
        "# Define a function to split DataFrame into chunks\n",
        "def split_dataframe(df, chunk_size):\n",
        "    chunks = []\n",
        "    num_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size > 0 else 0)\n",
        "    for i in range(num_chunks):\n",
        "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
        "    return chunks\n",
        "\n",
        "# Split the DataFrame if it exceeds the row limit\n",
        "row_limit = 30000\n",
        "sorted_by_cosine_chunks = split_dataframe(similarity_table.sort_values(by='CosineSimilarity', ascending=False).reset_index(drop=True), row_limit)\n",
        "sorted_by_url_chunks = split_dataframe(similarity_table.sort_values(by=['URL1', 'URL2'], ascending=[True, True]).reset_index(drop=True), row_limit)\n",
        "\n",
        "# Create and save the Excel files\n",
        "file_index = 1\n",
        "for cosine_chunk, url_chunk in zip(sorted_by_cosine_chunks, sorted_by_url_chunks):\n",
        "    output_file = f'internal_linking_opportunities{file_index}.xlsx'\n",
        "    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
        "        cosine_chunk.to_excel(writer, sheet_name='Sorted by Cosine Similarity', index=False)\n",
        "        url_chunk.to_excel(writer, sheet_name='Sorted by URL', index=False)\n",
        "    print(f\"File saved as {output_file}\")\n",
        "    files.download(output_file)\n",
        "    file_index += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "19K9DH71M5E0",
        "outputId": "0d6a0dc6-99a1-4563-fabc-135e22f45129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved as internal_linking_opportunities1.xlsx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_22938e04-6308-4bca-b63f-ee45a2f9406b\", \"internal_linking_opportunities1.xlsx\", 131763)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}